{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vMVo3TS2c3T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, accuracy_score, confusion_matrix, roc_curve\n",
        "from scipy.stats import zscore, pearsonr, uniform\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, RandomizedSearchCV\n",
        "\n",
        "from scipy.io import loadmat\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load in dataframes"
      ],
      "metadata": {
        "id": "fjPj9W2f3ygn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# categorical variable train dataframe\n",
        "\n",
        "file_path_trainC = \"/Users/caterinaponti/Desktop/WiDS/GlobalChallenge/TRAINING_CATEGORICAL.csv\"\n",
        "train_cat = pd.read_csv(file_path_trainC)\n",
        "print(train_cat.head())"
      ],
      "metadata": {
        "id": "ZL9Fy4Be2izZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_cat.columns"
      ],
      "metadata": {
        "id": "EsX0e0RU4jex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functional Connectome Matrices\n",
        "\n",
        "file_path_trainFCM = \"/Users/caterinaponti/Desktop/WiDS/GlobalChallenge/TRAIN_FCM.csv\"\n",
        "train_FCM = pd.read_csv(file_path_trainFCM)\n",
        "print(train_FCM.head())"
      ],
      "metadata": {
        "id": "s4NdiAR22oSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_FCM.columns"
      ],
      "metadata": {
        "id": "i22tDJMV4nYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantitative varaible train dataframe\n",
        "\n",
        "file_path_trainQ = \"/Users/caterinaponti/Desktop/WiDS/GlobalChallenge/TRAINING_QUANTITATIVE.csv\"\n",
        "train_Quant = pd.read_csv(file_path_trainQ)\n",
        "print(train_Quant.head())"
      ],
      "metadata": {
        "id": "yrNo0SP-2pz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_Quant.columns"
      ],
      "metadata": {
        "id": "YLHWTGKH4qBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ADHD and Sex solutions dataframe for model training\n",
        "\n",
        "file_path_trainS = \"/Users/caterinaponti/Desktop/WiDS/GlobalChallenge/TRAINING_SOLUTIONS.csv\"\n",
        "train_Solutions = pd.read_csv(file_path_trainS)\n",
        "print(train_Solutions.head())"
      ],
      "metadata": {
        "id": "mIXLPfwm6WGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_Solutions.columns"
      ],
      "metadata": {
        "id": "fwTsWx1x4r-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "wZAznwsu2RSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Data Analysis (EDA) is a way to understand your dataset by looking at its patterns, trends, and relationships. It helps identify missing data, outliers, and how variables interact."
      ],
      "metadata": {
        "id": "sjcRtC7Y6n2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `.info()` and `.describe()` to summarize each dataset."
      ],
      "metadata": {
        "id": "ehhDhkyc6xlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cat.info()"
      ],
      "metadata": {
        "id": "zKW7OdUu2WwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understand the distribution of the categorical variables with `.value_counts()`."
      ],
      "metadata": {
        "id": "fZ1Exwz44tpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Barratt_Barratt_P2_Occ - Barratt Simplified Measure of Social Status - Parent 2 Occupation\n",
        "train_cat['Barratt_Barratt_P2_Occ'].value_counts()\n",
        "\n",
        "#Look back at the dictionary [https://docs.google.com/spreadsheets/d/1wwmhLI4WPIX_jNzZTcSe69VpNiQh7cPW/edit?gid=1845471941#gid=1845471941]\n",
        "# to see what category these integers [0, 45, 35...] represent."
      ],
      "metadata": {
        "id": "T07otyOF6zq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize distributions:"
      ],
      "metadata": {
        "id": "ahQe57Ap63W5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='Barratt_Barratt_P2_Occ', data=train_cat[['Barratt_Barratt_P2_Occ']])\n",
        "plt.title(f\"Distribution of Barratt_Barratt_P2_Occ\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cDf5hyWh61wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understand the distribution of quantitative variables."
      ],
      "metadata": {
        "id": "zC_weFT968wK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of MRI_Track_Age_at_Scan\n",
        "train_Quant['MRI_Track_Age_at_Scan'].hist(figsize=(12, 10), bins=20)\n",
        "plt.suptitle(\"MRI_Track_Age_at_Scan Distributions\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fzSV4urF66qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JHuOnUp6xEAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train_Solutions dataset contains labels for `ADHD` and `gender`. Let's examine the class distribution."
      ],
      "metadata": {
        "id": "_q6gH6hY7B2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ADHD distribution\n",
        "train_Solutions['ADHD_Outcome'].value_counts()"
      ],
      "metadata": {
        "id": "9M0TurZv7Ch-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_Solutions['ADHD_Outcome'].value_counts().plot(kind='bar', color=['green', 'red'])\n",
        "plt.title('ADHD Outcome')\n",
        "plt.xlabel('Outcome (0 = No, 1 = Yes)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KZbnTsY_49sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gender distribution\n",
        "train_Solutions['Sex_F'].value_counts()"
      ],
      "metadata": {
        "id": "McPUzuLF7EAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Insight**: Gender imbalance in the dataset may introduce bias in modeling. Address this during data preparation."
      ],
      "metadata": {
        "id": "2ruorDbo5Awi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_Solutions['Sex_F'].value_counts().plot(kind='bar', color=['blue', 'orange'])\n",
        "plt.title('Gender Distribution')\n",
        "plt.xlabel('Gender (0 = Male, 1 = Female)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gyobHTO97IcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to Test Predictiveness:"
      ],
      "metadata": {
        "id": "nTMfBS6B5DzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Quantitative Data**: Use scatterplots to see trends (e.g., Color vision test score vs. ADHD)."
      ],
      "metadata": {
        "id": "GTOobk3t5IKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_Quant.columns"
      ],
      "metadata": {
        "id": "FwzIQbti5H5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the `SDQ_SDQ_Emotional_Problems`, which indicates Emotional Problems Scale."
      ],
      "metadata": {
        "id": "2CUkIRDR5N6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of the SDQ_SDQ_Emotional_Problems variable\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(train_Quant['SDQ_SDQ_Emotional_Problems'], kde=True, color='skyblue')\n",
        "plt.title('Distribution of SDQ_SDQ_Emotional_Problems')\n",
        "plt.xlabel('SDQ_SDQ_Emotional_Problems')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eopKX0z-5GoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This boxplot examines the relationship between `SDQ_SDQ_Emotional_Problems` and `ADHD_outcome` (as a target variable)."
      ],
      "metadata": {
        "id": "5D19iAhp5RsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for correlation with ADHD outcome\n",
        "train_Quant_copy = train_Quant.copy()\n",
        "train_Quant_copy['ADHD_Outcome'] = train_Solutions['ADHD_Outcome']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x='ADHD_Outcome', y='SDQ_SDQ_Emotional_Problems', data=train_Quant_copy)\n",
        "plt.title('SDQ_SDQ_Emotional_Problems vs ADHD Outcome')\n",
        "plt.xlabel('ADHD Outcome')\n",
        "plt.ylabel('SDQ_SDQ_Emotional_Problems')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ewAA9MDG5Zn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The boxplot reveals that individuals diagnosed with ADHD tend to have a higher mean score on the SDQ Emotional Problems scale compared to non-ADHD individuals. Additionally, the boxplot for the ADHD group displays greater variability, as evidenced by its extended range. This suggests that emotional problems are not only more pronounced but also more diverse within the ADHD group."
      ],
      "metadata": {
        "id": "dPt5cNhMVb5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Categorical Data**: Use bar plots or boxplots to compare groups (e.g., ADHD rates by gender)"
      ],
      "metadata": {
        "id": "9pK55jJq5bD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cat.columns"
      ],
      "metadata": {
        "id": "sPq83IBK5dHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at `Barratt_Barratt_P1_Edu` which indicates the Parent 1 level of education\n",
        "\n",
        "- 3=Less than 7th grade\n",
        "- 6=Junior high/Middle school (9th grade)\n",
        "- 9=Partial high school (10th or 11th grade)\n",
        "- 12=High school graduate\n",
        "- 15=Partial college (at least one year)\n",
        "- 18=College education\n",
        "- 21=Graduate degree"
      ],
      "metadata": {
        "id": "Nv-Yy3Ay5j7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.countplot(data=train_cat, x='Barratt_Barratt_P1_Edu', hue=train_Solutions['ADHD_Outcome'])\n",
        "plt.title('ADHD Prevalence by Parent 1 Education')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KxA38GXT5iJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_cat['Barratt_Barratt_P1_Edu'].value_counts()"
      ],
      "metadata": {
        "id": "Hk1eagtM5oLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the data points fall into a specific category (e.g., 21 has 470 entries out of a total 1213). This means that even if ADHD prevalence appears higher in this category, it might just reflect that there are more people in this group overall, rather than an actual trend."
      ],
      "metadata": {
        "id": "gx4Ws4Xh5l-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To adress this, normalize the data or compute percentages within each category to account for differences in group sizes. Let's compute ADHD percentage for each category."
      ],
      "metadata": {
        "id": "Q8sQcOa85sdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add ADHD_Outcome directly to a copy of the train_cat dataset for grouping\n",
        "train_cat_copy = train_cat.copy()\n",
        "train_cat_copy['ADHD_Outcome'] = train_Solutions['ADHD_Outcome']\n",
        "\n",
        "adhd_percentages = train_cat_copy.groupby('Barratt_Barratt_P1_Edu')['ADHD_Outcome'].mean()\n",
        "print(adhd_percentages)"
      ],
      "metadata": {
        "id": "ORDcWrBj5wMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categories like 3 - Less than 7th grade (80%) and 12 - High school graduate (72%) show some of the highest proportions of ADHD outcomes. Categories 21 - Graduate degree (67.2%) have relatively lower ADHD proportions compared to middle education levels."
      ],
      "metadata": {
        "id": "l-ENdlzD5zWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cat['Barratt_Barratt_P1_Edu'].value_counts()"
      ],
      "metadata": {
        "id": "oqA2mO12-my0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While these proportions might appear noteworthy, it is essential to consider the smaller sample size for lower education levels. **Smaller sample sizes can lead to greater variability** and make the proportions more susceptible to outliers. This means that a few individual cases can disproportionately affect the results, making them less reliable and less representative of the broader population. Therefore, it is crucial to examine sample sizes carefully before drawing any conclusions."
      ],
      "metadata": {
        "id": "mhAvHOIU-ipM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processed Categorical Columns"
      ],
      "metadata": {
        "id": "oRDNsCGc3uM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Our categorical columns include demographic data about the adolescent and parental information, such as occupation and education level.\n",
        "\n",
        "In the provided dataset, these categorical columns have been preprocessed by assigning numerical values to the categories within each variable. For instance, in the Parent 1 Occupation column, the number 35 might represent roles such as nurse, skilled technician, medical technician, or counselor. This grouping reduces the number of unique responses for each variable, simplifying the dataset.\n",
        "\n",
        "The integers currently representing categorical values can be cross-referenced in the provided data dictionary in [kaggle](https://www.kaggle.com/competitions/widsdatathon2025/data) to understand their corresponding categories.\n"
      ],
      "metadata": {
        "id": "msYji4jD2vYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What does One - Hot Encoding do ?"
      ],
      "metadata": {
        "id": "nKVPDyZMxPdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One hot encoding creates a new binary column for each unique category within a variable. For example, in the Parent 1 Occupation column with 10 possible categories (e.g., 0, 5, 10, 15, 20, etc.), one-hot encoding will generate 10 new columns: Parent_1_Occupation_0, Parent_1_Occupation_5, Parent_1_Occupation_10, and so on.\n",
        "\n",
        "Each of these new columns will contain boolean values (True or False). For instance, if a participant's parent 1 occupation falls into category 0, the Parent_1_Occupation_0 column will have a value of True, while the other columns for this variable will be False. The same logic applies to the other categories, ensuring each participant is appropriately represented in the dataset.\n"
      ],
      "metadata": {
        "id": "q_Lsp1cUxSL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why use one hot encoding\n",
        "\n",
        "Avoid Implying Ordinal Relationships\n",
        "\n",
        "\n",
        "1.  If you encode categories using numbers directly (e.g., 0, 1, 2), the algorithm might interpret these numbers as having a meaningful order or scale, which could lead to incorrect assumptions.\n",
        "2.  One-hot encoding eliminates this problem by assigning each category its own binary column, ensuring no ordinal relationship is implied.\n",
        "\n",
        "\n",
        "Improve Algorithm Performance\n",
        "\n",
        "\n",
        "*  Algorithms like logistic regression, decision trees, and neural networks often perform better with one-hot encoded data because it provides clear distinctions between categories.\n",
        "*  Without one-hot encoding, algorithms might struggle to learn from categorical variables or produce biased results.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ezgjKbLQMtOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One hot encoding is only possible on categorical variables so the first step is to switch our integer representations into category type variables."
      ],
      "metadata": {
        "id": "3YdmMwGsX73R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in train_cat.select_dtypes(include='int').columns:\n",
        "    train_cat[col] = train_cat[col].astype('category')"
      ],
      "metadata": {
        "id": "kngRrr5l2shv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our first column of our dataset is our participant id. This is an indicator variable that identifies each patient. This column is very important as it will be the one used to merge all of our data frames together, but, we do not want to encode this column. So, we will create a list of all the columns except the first one and label those `columns_to_encode`."
      ],
      "metadata": {
        "id": "hLMi9-JR3BSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of all of the columns except the first\n",
        "columns_to_encode = train_cat.columns[1:].tolist()\n",
        "\n",
        "# Print the columns to encode\n",
        "print(\"Columns to encode:\", columns_to_encode)"
      ],
      "metadata": {
        "id": "fvnHYTAT2-sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`pd.get_dummies(train_cat[columns_to_enccode], drop_first=True)`:\n",
        "* Converts selected categorical columns in train_cat into one-hot encoded columns creating binary (0 or 1) columns for each category.\n",
        "* The `drop_first=True` parameter avoids the \"dummy variable trap\" by dropping the first category for each feature,reducing redundancy in the encoded data.\n",
        "\n",
        "`data_encoded.applymap(lambda x: 1 if x is True else (0 if x is False else x))`:\n",
        "\n",
        "*   Iterates over every element in the data_encoded DataFrame and:\n",
        "  - Converts `True` to 1 and `False` to 0.\n",
        "  - Leaves all other values unchanged."
      ],
      "metadata": {
        "id": "VXiY7rIW3Nmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding categorical data\n",
        "data_encoded = pd.get_dummies(train_cat[columns_to_encode], drop_first=True)\n",
        "data_encoded = data_encoded.applymap(lambda x: 1 if x is True else (0 if x is False else x))"
      ],
      "metadata": {
        "id": "VemT5N343MX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reintroducing participant ID after encoding:"
      ],
      "metadata": {
        "id": "DE0FGpIQ39qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After encoding the categorical columns we will add back in the participant id column. The get dummies function will align the participants correctly with their respective feature variables because it respects the original index of the DataFrame, so row alignment is consistent. When concatenating `data_encoded` with the rest of the DataFrame `train_cat.drop(columns=columns_to_encode)`, the rows align because pandas automatically matches by index."
      ],
      "metadata": {
        "id": "4I6l3U-tRZwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine encoded columns with the rest of the DataFrame\n",
        "cat_train_final = pd.concat([train_cat.drop(columns=columns_to_encode), data_encoded], axis=1)\n",
        "\n",
        "# ensure it looks correct\n",
        "print(cat_train_final.head())"
      ],
      "metadata": {
        "id": "0VhPSB-H37Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Test Dataframes\n",
        "\n",
        "For any machine learning model you need training data and test data. On our Kaggle data page, you'll find both the training and testing dataframes. We have just encoded the categorical dataframe for our training data. Now, we need to apply the same encoding steps to the categorical dataframe in the testing data. It's essential to ensure that any preprocessing done on the training data is also applied to the test data to ensure accurate model predictions."
      ],
      "metadata": {
        "id": "LSppCDmW-3Zh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our test categorical dataframe is preprocessed the same way as our training data so we will follow the same steps to encode the dataframe."
      ],
      "metadata": {
        "id": "zjMEj1-pK-xJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load in test categorical dataframe\n",
        "\n",
        "file_path_testC = \"/Users/caterinaponti/Desktop/WiDS/GlobalChallenge/TEST_CATEGORICAL.csv\"\n",
        "test_cat = pd.read_csv(file_path_testC)\n",
        "#print(test_cat.head())"
      ],
      "metadata": {
        "id": "A8CrGqWwKUXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert our int variables to categories\n",
        "for col in test_cat.select_dtypes(include='int').columns:\n",
        "    test_cat[col] = test_cat[col].astype('category')\n",
        "\n",
        "# Creating a list of all of the columns except the first\n",
        "columns_to_encode = test_cat.columns[1:].tolist()\n",
        "\n",
        "# Print the columns to encode\n",
        "print(\"Columns to encode:\", columns_to_encode)\n",
        "\n",
        "# encoding categorical data\n",
        "data_encoded = pd.get_dummies(test_cat[columns_to_encode], drop_first=True)\n",
        "data_encoded = data_encoded.applymap(lambda x: 1 if x is True else (0 if x is False else x))\n",
        "\n",
        "# Combine encoded columns with the rest of the DataFrame\n",
        "cat_test_final = pd.concat([cat_test_final.drop(columns=columns_to_encode), data_encoded], axis=1)\n",
        "\n",
        "print(cat_test_final.head())"
      ],
      "metadata": {
        "id": "XlTawOGPKabu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging Data Frames\n",
        "\n",
        "Now we have categorical data frames for both our training and test datasets that are both ready to be merged with our remaining variables, the functional connectome matrices and quantitative data. The result of merging will be two final dataframes, train df and test df.\n",
        "\n",
        "All the dataframes include a participant_id variable, which serves as the unique identifier for each patient. This variable will be used to merge the dataframes, as it is the only common variable across the datasets.\n",
        "\n",
        "The pandas merge function can only merge two dataframes. So, I will first merge our encoded train categorical dataframe with our training functional connectome matrices. Then I will combine that merged data frame with our training quantitative dataframe. Again, this will all be executed using the pandas merge function on our participant id variable."
      ],
      "metadata": {
        "id": "i-_B5B_c4COr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cat_FCM = pd.merge(cat_train_final, train_FCM, on = 'participant_id')"
      ],
      "metadata": {
        "id": "YmdQCbME4R2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.merge(train_cat_FCM, train_Quant, on = 'participant_id')\n",
        "\n",
        "# ensure it looks accurate\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "id": "cCyUVJFm4Uk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This creates an almost complete train dataframe. You may have noticed that there is one dataframe left in our training data that we have not merged, the solutions. When training a machine learning model you have an `X_train ` dataframe containing the feature variables and a `Y_train` dataframe containing the target variables. We just created our `X_train` dataframe. In our third workshop we will cover how to use the `X_train` and `Y_train` dataframes to train the model.\n",
        "\n",
        "Our testing data does not contain solutions, since that is what you will produce when running your model. So we just need one feature variable dataframe.\n",
        "\n",
        "We will merge together the feature variables for our testing data, the same way we did for our training data. First, I will load in the quantative and functional connectome matrices.\n"
      ],
      "metadata": {
        "id": "LOAqavTWVWiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge test dataframes"
      ],
      "metadata": {
        "id": "-2b3i7Nq8Uq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_testFCM = \"/Users/caterinaponti/Desktop/WiDS/GlobalChallenge/TEST_FCM.csv\"\n",
        "test_FCM = pd.read_csv(file_path_testFCM)\n",
        "#print(train_FCM.head())\n",
        "\n",
        "file_path_testQ = \"/Users/caterinaponti/Desktop/WiDS/GlobalChallenge/TEST_QUANTITATIVE.csv\"\n",
        "test_Quant = pd.read_csv(file_path_testQ)\n",
        "#print(train_Quant.head())\n",
        "\n",
        "test_cat_FCM = pd.merge(cat_test_final, test_FCM, on = 'participant_id')\n",
        "\n",
        "test_df = pd.merge(test_cat_FCM, test_Quant, on = 'participant_id')\n",
        "\n",
        "# ensure it looks accurate\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "id": "RWr878CTV0VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have nearly complete train and test dataframes. There is one more step before we can perform machine learning, imputing NA values. Again, you will need to impute NA values the same way for both your training and test dataframes for accurate model performance."
      ],
      "metadata": {
        "id": "B9lYu5V09bSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NA values\n",
        "\n",
        "Before performing machine learning we must adress missing values to ensure optimal model performance.\n",
        "\n",
        "We will demonstrate how to fill the missing (`NA`) values using the mean of each column as an example.\n",
        "\n",
        "Note: There are many approaches to handle missing values, and the best method often depends on your dataset and chosen machine learning model. You can explore how your model deals with missing data and try alternative techniques. For example, this [website](https://www.widsworldwide.org/get-inspired/blog/a-data-scientists-deep-dive-into-the-wids-datathon/) has various ways to handle missing data.\n",
        "\n",
        "Filling NA values is a key challenge in this datathon to get accurate model performance. Experiment with different methods and evaluate which approach gives the best results for your model!"
      ],
      "metadata": {
        "id": "FPh-Mow14yTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check how many NA values we have\n",
        "print(train_df.isna().sum())\n",
        "\n",
        "# 371 NANs values\n",
        "# 360 in MRI_Track_age_at_Scan\n",
        "# 11 in PreInt_Demos_Fam_Child_Ethnicity"
      ],
      "metadata": {
        "id": "-pTwTsPx4uNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can fill the missing (`NA`) values in the columns `MRI_Track_Age_at_Scan` and `PreInt_Demos_Fam_Child_Ethnicity` individually by replacing them with the mean of their respective columns."
      ],
      "metadata": {
        "id": "4dKu_e5w5lU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.fillna({'MRI_Track_Age_at_Scan':train_df['MRI_Track_Age_at_Scan'].mean()}, inplace = True)\n",
        "train_df.fillna({'PreInt_Demos_Fam_Child_Ethnicity':train_df['PreInt_Demos_Fam_Child_Ethnicity'].mean()}, inplace = True)\n",
        "\n",
        "print(train_df.isna().sum().sum()) # should now be zero"
      ],
      "metadata": {
        "id": "nF82Zdhs5uE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use backward and forward fills. This indicates using previous or the following data row in order to replace the missing values.\n"
      ],
      "metadata": {
        "id": "kbF9iATIWnjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.bfill(inplace=True)\n",
        "print(train_df.isna().sum().sum())"
      ],
      "metadata": {
        "id": "SBTsNaK4WeiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have filled in our NA values our data frame is ready to be trained for machine learning. Tune into our next workshop to understand how to build your mulitoutcome machine learning model!"
      ],
      "metadata": {
        "id": "0keXkdRY50nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Outcome Machine Learning Model"
      ],
      "metadata": {
        "id": "xBde3bPk_Zj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A machine learning model is a mathematical model that, after being trained on a dataset, can make predictions or classifications on new data by identifying patterns within the information.\n",
        "\n",
        "Since this challenge is trying to answer the question:\n",
        "\n",
        "What brain activity patterns are associated with ADHD; are they different between males and females, and, if so, how?\n",
        "\n",
        "Our predictions or target variables in this model are gender (1 = female, 0 = male) and ADHD diagnosis (1 = has ADHD, 0 = does not). Both are classification problems, meaning the outputs are binary: each prediction will either be 0 or 1, representing a boolean result.\n",
        "\n",
        "To predict both target variables simultaneously, we will use a multi-outcome machine learning model. This type of model is designed to predict multiple dependent variables (or outcomes) at the same time, rather than one at a time. For this task, our Y_train dataset will include two target columns‚Äîgender and ADHD diagnosis. The predictions generated by your model, which you will submit to Kaggle, will also include these two variables. As a requirement for this challenge, you must use a multi-outcome model to produce predictions for both targets simultaneously."
      ],
      "metadata": {
        "id": "agzyRD-F_ik5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why use a Multi Outcome Model ?"
      ],
      "metadata": {
        "id": "fFLoThshtTE8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This approach is particularly relevant to our specific challenge of investigating ADHD diagnosis in women. Research suggests that women with ADHD are often underdiagnosed, and by modeling gender and ADHD diagnosis together, we can explore potential relationships between these variables. Using the same feature variables to predict both targets allows us to examine this connection in a systematic way. A multi-outcome model is ideal for this purpose because it efficiently handles the simultaneous prediction of both variables while leveraging any potential interdependencies between them.\n",
        "\n",
        "Additionally, using a multi-outcome model streamlines the workflow. We only need to preprocess the data once, and we can train a single model instead of separate models for each target. This not only saves time and computational resources but also simplifies the overall process, making it easier to maintain consistency across predictions for gender and ADHD diagnosis.\n",
        "\n"
      ],
      "metadata": {
        "id": "oYdHKgIOtWWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## X train and Y Train\n",
        "\n",
        "For any machine learning model, you need an X_train dataset, which contains the feature variables used to make predictions about the target variables‚Äîin this case, ADHD diagnosis and gender. Our feature variables include categorical columns, quantitative variables, and functional connectome matrices. We have prepared our X_train dataframe by encoding categorical variables, merging dataframes, and handling missing values by filling in NAs.\n",
        "\n",
        "In addition to X_train, we also need Y_train, the dataset containing the target variables, to train the model. The target variables allow the model to learn the patterns in the feature variables that lead to specific outcomes for ADHD diagnosis and gender. This relationship is what enables the model to make accurate predictions when applied to new data.\n",
        "\n",
        "So the first step we will take in coding is loading in the training solutions dataframe this will be our Y_train.\n"
      ],
      "metadata": {
        "id": "w_Fl7OeCtfi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_trainS = \"/Users/kylie/Documents/WiDs/Global_Challenge/TRAINING_SOLUTIONS.xlsx\"\n",
        "train_Solutions = pd.read_excel(file_path_trainS)"
      ],
      "metadata": {
        "id": "AebJNIbWVimK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both our X_train and Y_train datasets include the participant_id column, which serves as an identifier for each patient. Since this column is of type category and machine learning models require numerical data (int or float), it cannot be used during training. Additionally, it is not relevant for model learning. Therefore, we will remove the participant_id column from both dataframes before training the machine learning model."
      ],
      "metadata": {
        "id": "F2KeiNz5Adp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_df.drop(columns = ['participant_id'])\n",
        "Y_train = train_Solutions.drop(columns = ['participant_id'])"
      ],
      "metadata": {
        "id": "25tI1MNLBKGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost\n",
        "\n",
        "XGBoost, short for eXtreme Gradient Boosting, is an optimized implementation of gradient boosting, a machine learning technique that builds an ensemble of decision trees to make predictions. It is widely used for structured (tabular) data and is especially popular in competitive data science (e.g., Kaggle competitions) due to its high performance, speed, and flexibility.\n",
        "\n",
        "XGBoost builds an ensemble of decision trees in a sequential manner, where each tree corrects the errors of the previous ones. The final prediction is the weighted sum of the predictions from all trees.\n",
        "\n",
        "Gradient Boosting:\n",
        "\n",
        "Each tree tries to minimize the error (residual) from the previous step by fitting to the gradient of the loss function.\n",
        "This iterative process improves the model's accuracy over time.\n",
        "Ensemble Learning:\n",
        "\n",
        "By combining multiple weak learners (shallow decision trees), XGBoost creates a strong predictive model.\n"
      ],
      "metadata": {
        "id": "CWrlKucZtwpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we must create an instance of the XGBClassifier from the xgboost library.\n",
        "\n",
        "Key Parameters Explained:\n",
        "\n",
        "\n",
        "*   objective='binary:logistic': Specifies the task type as binary classification, with logistic regression as the objective function.\n",
        "*   n_estimators=100: Sets the number of boosting rounds (i.e., the number of trees to train in the ensemble).\n",
        "* learning_rate=0.1: Determines the step size shrinkage used to prevent overfitting. Lower values result in slower learning but can improve accuracy.\n",
        "* max_depth=5: Limits the depth of each decision tree to prevent overfitting while ensuring the model captures enough complexity."
      ],
      "metadata": {
        "id": "pf5NQ_D6s2pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Initialize the base classifier\n",
        "xgb_classifier = XGBClassifier(objective='binary:logistic', n_estimators=100, learning_rate=0.1, max_depth=5)"
      ],
      "metadata": {
        "id": "bZEGOcUjsVvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will wrap the XGBClassifier with MultiOutputClassifier, a utility from sklearn that enables the model to handle multi-target classification.\n",
        "\n",
        "Why it's needed:\n",
        "XGBClassifier on its own can only handle single-target classification. MultiOutputClassifier manages the training process by creating a separate XGBClassifier for each target variable in y_train."
      ],
      "metadata": {
        "id": "Yi3LZAynt394"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap with MultiOutputClassifier for multi-target classification\n",
        "multioutput_classifier = MultiOutputClassifier(xgb_classifier)"
      ],
      "metadata": {
        "id": "Y6kVXgvus0l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we must train the multioutput_classifier on the feature matrix X_train and the multi-target labels y_train.\n",
        "\n",
        "How it works:\n",
        "\n",
        "MultiOutputClassifier internally splits y_train into individual target columns.\n",
        "For each target column, it trains a separate instance of the XGBClassifier using the features from X_train."
      ],
      "metadata": {
        "id": "1v4UiSc3uCVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "multioutput_classifier.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "RTQjS1KLuB7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Model\n",
        "\n",
        "Once we have a trained model we can start testing our model and making our predictions! We also preprocessed our testing data the same way we did our training data so we are all ready to use our test_df as our x_test dataframe in our trained model.\n",
        "\n",
        "Participant_id: we will remove this column because it is type category and not helpful in machine learning but we will store it in a variable to save it to concat back to our dataframe later before submitting your predicitions"
      ],
      "metadata": {
        "id": "9OHE7y3TuTLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "participant_id = test_df['participant_id']\n",
        "\n",
        "X_test = test_df.drop(columns = 'participant_id')\n",
        "\n",
        "y_pred = multioutput_classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "VbTYEBEsu1ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create a dataframe of the participant ids that we stored and our predicted values"
      ],
      "metadata": {
        "id": "z1Hkpx9IFYEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions to a DataFrame\n",
        "predictions_df = pd.DataFrame(\n",
        "    y_pred,\n",
        "    columns=['Predicted_Gender', 'Predicted_ADHD']\n",
        ")\n",
        "\n",
        "# Combine participant IDs with predictions\n",
        "result_df = pd.concat([participant_id.reset_index(drop=True), predictions_df], axis=1)\n",
        "\n",
        "# Print or save the DataFrame\n",
        "print(result_df)"
      ],
      "metadata": {
        "id": "T-fo5pslHmE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of an expected result that you will submit to kaggle:"
      ],
      "metadata": {
        "id": "Z0Ff02LkKz7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    participant_id  Predicted_Gender  Predicted_ADHD\n",
        "0     Cfwaf5FX7jWK                 0               0\n",
        "1     vhGrzmvA3Hjq                 1               0\n",
        "2     ULliyEXjy4OV                 1               0\n",
        "3     LZfeAb1xMtql                 0               0\n",
        "4     EnFOUv0YK1RG                 1               0\n",
        "..             ...               ...             ...\n",
        "299   UadZfjdEg7eG                 0               0\n",
        "300   IUEHiLmQAqCi                 1               1\n",
        "301   cRySmCadYFRO                 0               0\n",
        "302   E3MvDUtJadc5                 0               0\n",
        "303   dQJXfyRazknD                 1               0"
      ],
      "metadata": {
        "id": "VQzvft4TFkbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Machine Learning Model - Cross Validation Score\n",
        "\n",
        "The cross-validation score is a metric used to evaluate the performance of a machine learning model by testing it on multiple subsets of the data. It provides an estimate of how well the model is likely to generalize to unseen data.\n",
        "\n",
        "How Cross-Validation Works:\n",
        "\n",
        "\n",
        "*   The dataset is divided into ùëò equally-sized subsets or folds.\n",
        "*   The model is trained on k - 1 folds (the training set) and evaluated on the remaining fold (the validation set)\n",
        "* This process is repeated k times, with each fold being used once as the validation set\n",
        "* The evaluation metric, we will use accuracy is computed for each iteration\n",
        "\n",
        "Why are we using cross-validation score:\n",
        "\n",
        "\n",
        "*  Since we don‚Äôt have the true labels for the test data, we cannot calculate the actual accuracy score. Cross-validation provides a reliable way to evaluate our model's performance using the training data without needing the test set answers.\n",
        "*  Cross-validation ensures that a model is evaluated on multiple subsets of the data\n",
        "\n"
      ],
      "metadata": {
        "id": "h8rzvf-RIWLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary libraries: We will be using the sklearn library in python to perform cross validation"
      ],
      "metadata": {
        "id": "ya35G8HlU6Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, accuracy_score"
      ],
      "metadata": {
        "id": "K3cIe1fgIb0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a custom accuracy metric for multi-output classification, where each target variable (column) needs a separate accuracy calculation.\n",
        "\n",
        "This function will:\n",
        "\n",
        "\n",
        "*   Converts y_true and y_pred into NumPy arrays to enable indexing like `[:, i].`\n",
        "*   Iterates over each column (i.e., target variable) in y_true and y_pred.\n",
        "* Computes the accuracy score for each target variable using accuracy_score.\n",
        "* Returns the average accuracy across all target variables."
      ],
      "metadata": {
        "id": "KiJuwha9VZr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_output_accuracy(y_true, y_pred):\n",
        "    # Ensure y_true and y_pred are NumPy arrays\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    # Compute accuracy for each target variable and return the mean\n",
        "    return np.mean([accuracy_score(y_true[:, i], y_pred[:, i]) for i in range(y_true.shape[1])])"
      ],
      "metadata": {
        "id": "O8yEjiwjVKY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will wrap the multi_output_accuracy function into a scorer object that can be used with scikit-learn's evaluation tools (e.g., cross_val_score). This will automatically create the `y_true` and `y_pred` being used in our function above."
      ],
      "metadata": {
        "id": "-HD_jPNnVxdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scorer using scikit-learn's make_scorer\n",
        "multi_output_scorer = make_scorer(multi_output_accuracy)"
      ],
      "metadata": {
        "id": "s-QjhupUVw1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will actually perform cross validation with our multi-outcome machine learning model while using our X_train and Y_train dataframes.\n",
        "\n",
        "*   `cv=5`: Specifies 5-fold cross-validation so it will split X_train and Y_train into 5 subsets. Train the model on 4 folds and evaluates it on the remaining fold. Then, repeat this process 5 times, with each fold being used as the validation set once.\n",
        "*   `scoring = multi_output_scorer`: Uses the custom scoring function for evaluation."
      ],
      "metadata": {
        "id": "i4g3d0UxWJVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform cross-validation on the training data\n",
        "cv_scores = cross_val_score(multioutput_classifier, X_train, Y_train, cv=5, scoring=multi_output_scorer)\n",
        "\n",
        "# Output the cross-validation results\n",
        "print(\"Cross-validation scores for each fold:\", cv_scores)\n",
        "print(\"Mean CV score:\", np.mean(cv_scores))\n",
        "\n",
        "# Cross-validation scores for each fold: [0.82304527 0.78600823 0.69341564 0.64669421 0.33471074]\n",
        "# Mean CV score: 0.6567748188960311"
      ],
      "metadata": {
        "id": "p1DsEkcyWDxt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}